{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm as tqdm # Explain loops\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.un.org//en/story/2004/09/116712-concerns-island-nations-top-final-session-un-assemblys-high-level-debate\n"
     ]
    }
   ],
   "source": [
    "with open(\"../files/url_list.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "urls = text.splitlines()\n",
    "\n",
    "df= pd.read_csv('../files/articles.csv')\n",
    "\n",
    "for url in urls[1284:1285]:\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url, timeout= 25)\n",
    "        sleep(random.uniform(3,4))\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                title = soup.find('h1').find('span', class_ = \"field field--name-title field--type-string field--label-hidden\").text\n",
    "            except:\n",
    "                title = np.nan\n",
    "\n",
    "            try:    \n",
    "                date = soup.find_all('span', class_ = \"views-field views-field-field-news-date\")[0].find('span', class_ = \"field-content\").find('time', class_ = \"datetime\").text\n",
    "            except:\n",
    "                date = np.nan\n",
    "\n",
    "            try:\n",
    "                category = soup.find_all('span', class_ = \"views-field views-field-field-news-topics\")[0].find('span', class_ = \"field-content\").text\n",
    "            except:\n",
    "                category = np.nan\n",
    "\n",
    "            try:\n",
    "                summary = soup.find_all('div', class_ = \"views-field views-field-field-news-story-lead\")[0].find('p').text\n",
    "            except:\n",
    "                summary = np.nan\n",
    "\n",
    "            try:\n",
    "                temp_text_div = soup.find_all('div', class_ = \"clearfix text-formatted field field--name-field-text-column field--type-text-long field--label-hidden field__item\")[0]\n",
    "                temp_text_p_list = temp_text_div.find_all('p')\n",
    "\n",
    "                temp_text_list = []\n",
    "                for p in temp_text_p_list:\n",
    "                    temp_text_list.append(p.text)\n",
    "\n",
    "                temp_text = ' \\a '.join(temp_text_list).replace(\"\\xa0\", \"\")\n",
    "            except:\n",
    "                temp_text = np.nan\n",
    "\n",
    "            try:\n",
    "                temp_text_div = soup.find_all('div', class_ = \"clearfix text-formatted field field--name-field-text-column field--type-text-long field--label-hidden field__item\")[0]\n",
    "                temp_text_h3_list = temp_text_div.find_all('h3')\n",
    "\n",
    "                temp_h3_list = []\n",
    "                for h3 in temp_text_h3_list:\n",
    "                    temp_h3_list.append(h3.text)\n",
    "\n",
    "                h3_text = ' \\a '.join(temp_h3_list).replace(\"\\xa0\", \"\")\n",
    "            except:\n",
    "                h3_text = np.nan\n",
    "            link = url\n",
    "            temp_dict = {\n",
    "                'link' : link,\n",
    "                'title': title,\n",
    "                'date' : date,\n",
    "                'category' : category,\n",
    "                'summary' : summary,\n",
    "                'main_text': temp_text,\n",
    "                'headers' : h3_text\n",
    "            }\n",
    "            temp_df = pd.DataFrame(temp_dict.values()).T\n",
    "            temp_df.columns = temp_dict.keys()\n",
    "            df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    except:\n",
    "        print('Conection Error')\n",
    "        df.to_csv('../files/articles.csv', index=False)\n",
    "df.to_csv('../files/articles.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
